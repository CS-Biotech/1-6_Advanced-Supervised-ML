{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rawbBiJk2m3C"
   },
   "source": [
    "# Week 6 - Advanced Supervised Learning\n",
    "Last week, we explored the fundamentals of machine learning, concentrating on model training and evaluation. While this laid the groundwork for constructing an ML pipeline, we did not address methods for enhancing model performance or advanced models for more complex tasks. This week, we will introduce a new model and delve deeper into configuring and evaluating models.\n",
    "\n",
    "By the end of this module, you should be able to:\n",
    "1. Explain the components and hyperparameters of a decision tree\n",
    "2. Explain underfitting and overfitting.\n",
    "3. Understand the use of validation datasets.\n",
    "4. Implement a program to tune the hyperparameters of a machine learning model to improve its predictive performance.\n",
    "\n",
    "### Pima Diabetes Case Study.\n",
    "We will continue working with the Pima Diabetes Dataset. Below we import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-8vfTj3ShtU"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we fetch the dataset from https://www.openml.org/search?type=data&status=active&id=37\n",
    "X,y = fetch_openml(data_id = 37, as_frame = True, return_X_y = True)\n",
    "\n",
    "# convert tested_positive and tested_negative to 1 and 0\n",
    "y = (y == 'tested_positive').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gN65yxTTd8o"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q1: Split the dataset into a Training and Testing dataset.**\n",
    "\n",
    "> HINT: Look at Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cpYVS1OTo4C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### Your Code Here.\n",
    "## NOTE: please use X_train, X_test, y_train, and y_test as your variable names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALXuF0b8T1ZO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQXlPmdWvwXb"
   },
   "source": [
    "## Training a Decision Tree\n",
    "In the pre-module, we introduced the concept of a decision tree and discussed how to interpret it.. Now, let's explore how to train a decision tree using our data. Recall the training procedure for a ML model from Week 5:\n",
    "\n",
    "\n",
    "1. We input all data points from the dataset \\( $x$ \\) into the model.\n",
    "2. The model processes this input, performing computations based on \\( $x$ \\) and its current weights, to produce predictions, denoted as \\( $\\hat{y}$ \\).\n",
    "3. We then calculate the error (the opposite of accuracy) for all predictions made by the model.\n",
    "4. By analyzing the model's correct and incorrect predictions, we adjust the relevant parameters to improve the accuracy of subsequent predictions.\n",
    "5. We then repeat steps 1-4 until the error ceases to decrease or predefined stopping conditions are met (e.g., we hit a limit on the number of iterations).\n",
    "\n",
    "Decision Trees are trained in a very similar manner, starting with the root node. The algorithm can be summarized as follows:\n",
    "\n",
    "1. We input all data points from the dataset \\( $x$ \\) into the model.\n",
    "2. Using these datapoints, we create a set of candidate decision rules (`feature <= some value`) by looking at every possible combination of input features and feature values.\n",
    "3. We evaluate the effectiveness of each candidate rule in classifying the data.\n",
    "4. 4. The selected rule is then added to our decision tree, forming a new decision node and corresponding leaf nodes.\n",
    "5. We then repeat steps 1-4 until the error no longer decreases or a stopping condition is met.\n",
    "6. We assign each leaf node a class based on the majority class that reaches the node.\n",
    "\n",
    "\n",
    "![fkiw](dtree_flow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm, there are two main questions we need to answer:\n",
    "\n",
    "1. How do we calculate how good a rule is?\n",
    "2. How do we know when to stop?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VrTzD42JSSQ"
   },
   "source": [
    "### Impurity:\n",
    "We can measure how good a rule is by measuring the **Impurity** of the resultant groups.\n",
    "\n",
    "Impurity measures the amount of uncertainty present in the data. Intuitively for a classification problem, we can understand this as \"if we were to randomly select a point, how confident are we that we know what the point will be.\"\n",
    "\n",
    "Consider the following groups:\n",
    "\n",
    "![name](groupab.png)\n",
    "\n",
    "\n",
    "If we were to randomly select a data point from Group A, we have a 2/3 chance of the point being green and a 1/3 chance of it being purple, therefore we can say we are relatively confident that a point chosen from Group A will be green. However, in Group B, there is a 50/50 chance that the point will be green or purple, therefore we are not confident that we can predict the result of randomly choosing a point. In this case, we can say that Group B is less pure or has *higher impurity*\n",
    "\n",
    "**A group has zero impurity if there is only one class in the group.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZBhwb6hjI8T"
   },
   "source": [
    "#### Splitting with Impurity\n",
    "\n",
    "A decision tree utilizes impurity to guide its decision-making process. At each internal node, the tree evaluates the data points that have reached that node and selects a decision rule that creates two subgroups with minimal impurity. Consider the following example classification:\n",
    "\n",
    "The dataset contains two classes (orange and blue). Based on these points, the decision tree generates several candidate decision rules to partition the space. **The tree then selects the rule that most effectively reduces impurity.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClhW-wPHIDvc"
   },
   "source": [
    "---\n",
    "##### **Q2: Below we plot two different candidate splits. Which split will the decision tree choose: Candidate 1 or Candidate 2? Justify your answer**\n",
    "\n",
    "![splits](splits.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bvOTi6dJLSN"
   },
   "source": [
    "*Your Answer Here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPxhiGwN8Vz5"
   },
   "source": [
    "### When to Stop\n",
    "\n",
    "There are various conditions in which we can stop creating new splits in a decision tree, but most of the time we focus on the following four:\n",
    "\n",
    "1. **Maximum Tree Depth:** The first condition occurs when the newly added node reaches the maximum allowable depth.\n",
    "2. **Not Enough Samples**: The second condition arises when there are not enough data points to create two splits with a sufficient number of points. For instance, if one of the splits contains only a single sample, the split may not be executed.\n",
    "3. **No Reduction in Impurity**: TThe third condition is met when the resulting groups from the split are already highly pure, and no further reduction in impurity can be achieved by adding a new split.\n",
    "4. **No further Splits Possible**: The tree will cease training if all samples are perfectly classified or if the misclassified samples cannot be separated from other samples (e.g., two samples with identical values but different classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdfQPEkhVcnt"
   },
   "source": [
    "### Fitting a Decision Tree\n",
    "Putting everything together, we fit a decision tree onto our training set in the cell below. Like `LogisticRegression`, we don't need to code the Decision Tree training procedure ourselves. Instead, we can simply use the `DecisionTreeClassifier` from `sklearn.tree`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK-GWpNpV9Dc"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an instance of the Decision Tree model.\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth = 3, # Condition 1: Stop creating new nodes when we reach a depth of 3\n",
    "    min_samples_split = 4, # Condition 2: Don't create a new node if there are less than 4 samples in a node.\n",
    "    min_impurity_decrease = 0.02, # Condition 3: Each node needs to reduce impurity by least 0.05\n",
    ")\n",
    "\n",
    "# Fit the model to the training data.\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "train_predictions = dt.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfWJ5JqXYfOC"
   },
   "source": [
    "Note, while we specify `max_depth,` `min_impurity_decrease` and `min_samples_split` here, if we do not specify a value then the tree will follow condition 4 and keep adding nodes till theres nowhere to go (condition 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaWxSJpVX677"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q3: Use the model to make predictions on the test set. What is the accuracy on the test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1Rrp24nX7Fj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "### Your Code here\n",
    "\n",
    "### Make predictions on the test set\n",
    "\n",
    "### Calculate test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qeQcvCQapsI"
   },
   "source": [
    "### Visualizing Our Tree\n",
    "\n",
    "One benefit of decision trees is that we can visualize the trained tree to gain insights into our data. Below we use the `plot_tree` function from `sklearn.tree` to visualize our model. This function takes in:\n",
    "1. The trained decision tree\n",
    "2. The names of the features (`feature_names`)\n",
    "3. The names of the classes (`class_names`)\n",
    "4. Flags to control what the visualization outputs including whether to color the boxed (`filled`) and display the impurity of the nodes (`impurity`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxq1hSIgaon6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "# increase figure size\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# run the plot_tree command\n",
    "plot_tree(dt, # pass in the trained model\n",
    "          feature_names = X.columns, # provide the feature names\n",
    "          class_names = ['tested_negative', 'tested_positive'], # provide the class names\n",
    "          filled = True, # color the nodes\n",
    "          impurity = False # do not show impurity values (makes tree easier to read)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvKDZcp0X7Mz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fudUfmAxSydP"
   },
   "source": [
    "## Overfitting and Underfitting\n",
    "In Week 5, we introduced the concept of splitting the data in order to better evaluate how the model might perform on real-world data. As a reminder, we have two sets of data:\n",
    "\n",
    "1. Training set: This is the dataset we give the model to learn from. Think of it as homework given to students. The model can practice getting these answers correctly (aka training).\n",
    "2. Test set: This is data the model has not seen. Think of it as the exam. Student's used homework questions to learn the concepts (training), now we evaluate how well they learned patterns and trends by giving them questions they have not\n",
    "\n",
    "This week, we will delve more into the reason *why* a model may underperform at a task. Specifically, a model performs poorly in situations when the model is **underfitting** and **overfitting**.\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and unseen data. For example, a decision tree can underfit when there are not enough decision nodes since it cannot make enough decisions to adequately distinguish between the classes.\n",
    "\n",
    "**Overfitting** happens when a model learns the training data too well, including its noise and small details, making it overly complex. As a result, the model performs great on the training data but poorly on unseen data because it fails to capture the broader patterns. For example, if a decision tree is too deep (too many decisions), each point could potentially end up in its own leaf, causing the model to \"memorize\" the training data instead of generalizing.\n",
    "\n",
    "### Controlling for Overfitting and Underfitting\n",
    "So how do we control overfitting and underfitting? Simple: We configure the model's **Hyperparameters**. Hyperparameters are basically the knobs and levers of the model. They are parameters the user sets to control how the model learns. For example, in Logistic Regression, `max_iter` was a hyperparameter controlling the number of training iterations. Similarly, Decision Trees also have hyperparameters you have already seen: `max_depth`, `min_samples_split`, and `min_impurity_decrease`.\n",
    "\n",
    "* `max_depth`: The first parameter used to control overfitting is `max_depth`. The max_depth specifies the number of sequential decisions that can lead to a leaf that the decision tree model cannot exceed, and no additional decisions will be made even if the model performance is poor. Since every extra depth of the tree doubles the number of leaves, a sufficiently deep tree can easily assign each training sample to its own leaf. This over-complexity has the tendency to fit onto noise instead of actual trends.\n",
    "\n",
    "* `min_samples_split`: Another way to avoid overfitting is to specify that there must be more than one point in each leaf. The value set for this parameter will determine the number of samples that must remain at the end of each leaf. This will force the tree to focus on patterns that appear multiple times, making it more robust to noise. This also helps very deep trees from assigning 1 leaf per point.\n",
    "\n",
    "* `min_impurity_decrease`: While the goal of each node is to reduce the impurity in each split, sometimes to promote generalization, we only want our decision tree to consider splits that are \"strong\" enough, i.e. reduce impurity enough. Therefore setting this parameter will prevent the decision tree model from using any decision rules that lead to a decrease in purity that is less than the set value. This helps in reducing overfitting to noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXhSaCg8fcFV"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q4: Train a Decision Tree with `max_depth` values between 1 and 4.  Record the train and test accuracy for each tree. At what depth do you think the tree starts to overfit? At what depths does the tree underfit? Justify your answer.**\n",
    "\n",
    "> HINT: You can use a for loop to iterate over the `max_depth` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxvXzkqSfccb"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6gA8Aoufcmk"
   },
   "source": [
    "*Your Answer Here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH7wrXZKdwrg"
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "While hyperparameters are crucial to a model's performance, we have yet to address the key question: *How do we select the appropriate hyperparameters?*\n",
    "\n",
    "One might consider testing numerous hyperparameter configurations and choosing the one that yields the best performance on the test set. However, this approach has a fundamental flaw: the test set is intended to represent unseen data. To illustrate, consider the analogy of a machine learning model as a student: the training set samples are homework problems, while the test set is the exam. Selecting hyperparameters based on test set performance is akin to devising a study strategy, taking the exam, receiving the score, adjusting the study strategy, and retaking the same exam repeatedly. Although this may result in a high test score, it provides an overly optimistic assessment that does not accurately reflect the student's true understanding of the subject matter. Similarly, using the test set to determine the best hyperparameters leads to an overly optimistic estimate of the model's real-world performance.\n",
    "\n",
    "So, what should we do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq0Pp48C7llS"
   },
   "source": [
    "\n",
    "### K-Fold Cross-Validation\n",
    "Well, like studying for the exam, we can use *practice* exams, sets of questions that are similar to the exam (but not the same) to evaluate our performance. We can create these practice exams by randomly selecting homework problems and setting them aside. We can then use these practice exams to evaluate our studying strategy.\n",
    "\n",
    "In ML, these practice exams are called *validation* data sets and we construct these sets by taking the training dataset, and further randomly split it up into $K$ relatively-equal sized chunks, or \"Folds\". We will then create a new version of our model, train it using all but one of the folds, and test it on the remaining fold. We will then rotate the folds and repeat this process. At the conclusion, we will average the performance across all folds. This method is known as **K-Fold Cross-Validation.** We will apply this procedure to various hyperparameter configurations\n",
    "\n",
    "![gscv](grid_search_cross_validation.png)\n",
    "\n",
    "We can create these folds using the `KFold()` utility from `sklearn.model_selection.` This takes a dataset, assigns samples to different folds, and provides the indices of what samples are in each fold. Below we create 5 Folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XeVQEzLkiCV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5,\n",
    "           shuffle = True) # make sure you set shuffle to true\n",
    "fold_indices = kf.split(X_train)\n",
    "for fold_index in fold_indices:\n",
    "    # Get what samples should be in each split\n",
    "    train_fold_indices, val_fold_indices = fold_index\n",
    "\n",
    "    # select the samples\n",
    "    X_train_fold = X_train.iloc[train_fold_indices]\n",
    "    y_train_fold = y_train.iloc[train_fold_indices]\n",
    "    X_val_fold = X_train.iloc[val_fold_indices]\n",
    "    y_val_fold = y_train.iloc[val_fold_indices]\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Number of samples in training fold: {len(train_fold_indices)}\")\n",
    "    print(f\"Number of samples in validation fold: {len(val_fold_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdSLTXJPrtFM"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q5: Below we create the procedure for testing one hyperparameter configuration of a decision tree. Fill in the blank pieces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXxsVf4FrtNV"
   },
   "outputs": [],
   "source": [
    "def test_hyperparameter(X_train, y_train, max_depth, min_samples_split, min_impurity_decrease):\n",
    "  # create folds\n",
    "  kf = KFold(n_splits = 5)\n",
    "  fold_indices = kf.split(X_train)\n",
    "  accuracies = []\n",
    "  # loop through the folds\n",
    "  for fold_index in fold_indices:\n",
    "    # Get what samples should be in each split\n",
    "    train_fold_indices, val_fold_indices = fold_index\n",
    "    # select the samples\n",
    "    X_train_fold = X_train.iloc[train_fold_indices]\n",
    "    y_train_fold = y_train.iloc[train_fold_indices]\n",
    "    X_val_fold = X_train.iloc[val_fold_indices]\n",
    "    y_val_fold = y_train.iloc[val_fold_indices]\n",
    "\n",
    "\n",
    "    # initialize a new decision tree\n",
    "    dt = ... # YOUR CODE HERE\n",
    "\n",
    "    # Train the decision tree on the train fold\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "  \n",
    "    # Make predictions on the validation fold\n",
    "    val_predictions  = ... # YOUR CODE HERE\n",
    "\n",
    "    # calculate the accuracy of the val predictions\n",
    "\n",
    "    fold_accuracy = ... # YOUR CODE HERE\n",
    "\n",
    "    accuracies.append(fold_accuracy)\n",
    "  return np.mean(accuracies) # return the average accuracy across all folds\n",
    "\n",
    "\n",
    "## Run the function with a set of parameters\n",
    "cv_acc = test_hyperparameter(X_train, y_train,\n",
    "                             max_depth = 3,\n",
    "                             min_samples_split = 3,\n",
    "                             min_impurity_decrease = 0.01)\n",
    "\n",
    "print(f\"The average cross-validation accuracy is: {cv_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DvY7p3WrtkY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1q3KECf7myO"
   },
   "source": [
    "### Parameter Grid Search\n",
    "\n",
    "Having established the correct method for evaluating hyperparameters, we now need to identify the optimal parameters. This is accomplished through a process known as 'Grid Search,' where we test every possible combination of parameters to determine which configuration yields the best cross-validation score. Below, we test three different values of `max_depth`, `min_samples_split`, and `min_impurity_decrease` each for a total of $3 \\times 3 \\times 3 = 27$ combinations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM9GoA4j9tiq"
   },
   "outputs": [],
   "source": [
    "for max_depth in [1,2,3]:\n",
    "  for min_samples_split in [2,3,4]:\n",
    "    for impurity in [0.00, 0.01, 0.02]:\n",
    "      cv_acc = test_hyperparameter(X_train, y_train,\n",
    "                                   max_depth = max_depth,\n",
    "                                   min_samples_split = min_samples_split,\n",
    "                                   min_impurity_decrease = impurity)\n",
    "      print(f\"max_depth: {max_depth}, min_samples_split: {min_samples_split}, min_impurity_decrease: {impurity}, cv_acc: {cv_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8IT8fCg-G1M"
   },
   "source": [
    "After finding the best set of parameters, we then train the model on *all* the training data and do our final evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb-1C-8L-Rnk"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q6: What set of parameters had the best CV accuracy? Train a model using these parameters using the entire training set. Report the training and testing accuracy.**\n",
    "\n",
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5j9XCBp8-G6e"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWhbwYtC-HAK"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_q_LHuz_YKF"
   },
   "source": [
    "## Graded Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhFQ3T1c_Ys0"
   },
   "source": [
    "---\n",
    "\n",
    "##### **GQ1: Read in the heart failures dataset (`hf_data_tut.csv`) (1pt). Split the predictor variables (features) from the labels (response variable) (1pt), and create a training and testing set (1pt). **\n",
    "\n",
    "> HINT: Look at your work from Week 3 and 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlD5XdvE_a9J"
   },
   "outputs": [],
   "source": [
    "## Your Code Here\n",
    "\n",
    "import pandas as pd\n",
    "#import the data set\n",
    "\n",
    "#assign a portion of the data to the response variable\n",
    "y = ...\n",
    "\n",
    "#assign a portion of the data as predictor variables\n",
    "X = ...\n",
    "\n",
    "#create traning and testing data subsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNT5bjiB_bA0"
   },
   "source": [
    "---\n",
    "##### **GQ2: Train a decision tree of Depth 2 and visualize the tree (1pt). What feature is used in the root node (1pt)? How many leaves does the tree have (1pt)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LGtV68b_gMO"
   },
   "outputs": [],
   "source": [
    "## Your Code Here\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Setup decision tree, specify the depth\n",
    "dt = ...\n",
    "\n",
    "# fit your data to the decison tree\n",
    "\n",
    "# plot your decision tree\n",
    "\n",
    "# increase figure size\n",
    "\n",
    "# run the plot_tree command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwZIK6ZuA6QG"
   },
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv9CbQn7Ayjd"
   },
   "source": [
    "---\n",
    "##### **GQ3: Print the testing and training accuracy of the model above (1pt). Do you think the model is under or overfitting (or neither) and why (2pt)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR0eUDh9A40M"
   },
   "outputs": [],
   "source": [
    "## Your Code Here\n",
    "\n",
    "train_pred = ...\n",
    "test_pred = ...\n",
    "train_accuracy = ...\n",
    "test_accuracy = ...\n",
    "print(f\"Train Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVm2Csr_A44z"
   },
   "source": [
    "Train Acc is significantly higher than test accuracy, therefore the model is probably overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs0rKJsC_gP6"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In conclusion, decision trees are powerful and intuitive tools in machine learning, offering both flexibility and interpretability. By iteratively splitting data based on feature values, decision trees can model complex relationships and make predictions for both classification and regression tasks. Additionally, their ability to be visualized and easily interpreted makes them a valuable analytical tool.\n",
    "\n",
    "However, with this flexibility comes the risk of overfitting, where the model becomes too tailored to the training data and loses its ability to generalize to new data. To prevent this, various parameters can be tested using K-Fold Cross Validation to find the best configuration. With this knowledge, you can confidently apply ML to a wide range of problems, making them a valuable tool in your machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
