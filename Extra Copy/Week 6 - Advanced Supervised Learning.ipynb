{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rawbBiJk2m3C"
   },
   "source": [
    "# Week 6 - Advanced Supervised Learning\n",
    "Last week, we covered the basics of ML, mainly focusing on how a model is trained and evaluated. While this provides the foundations of how to construct an ML pipeline, we haven't covered on how to improve your ML model's performance nor did we cover any advanced models that may be used for more difficult tasks. This week, we introduce a new model as well as go more into detail about configuring and evaluating models. \n",
    "\n",
    "By the end of this module, you should be able to:\n",
    "1. Explain the components and hyperparameters of a decision tree\n",
    "2. Explain underfitting and overfitting.\n",
    "3. Understand the use of validation datasets.\n",
    "4. Implement a program to tune the hyperparameters of a machine learning model to improve its predictive performance.\n",
    "\n",
    "### Pima Diabetes Case Study.\n",
    "We will continue working with the Pima Diabetes Dataset. Below we import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-8vfTj3ShtU"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we fetch the dataset from https://www.openml.org/search?type=data&status=active&id=37\n",
    "X,y = fetch_openml(data_id = 37, as_frame = True, return_X_y = True)\n",
    "\n",
    "# convert tested_positive and tested_negative to 1 and 0\n",
    "y = (y == 'tested_positive').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gN65yxTTd8o"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q1: Split the dataset into a Training and Testing dataset.**\n",
    "\n",
    "> HINT: Look at Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cpYVS1OTo4C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### Your Code Here.\n",
    "## NOTE: please use X_train, X_test, y_train, and y_test as your variable names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALXuF0b8T1ZO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQXlPmdWvwXb"
   },
   "source": [
    "## Training a Decision Tree\n",
    "In the pre-module we introduced the concept of a decision tree and discussed how to interpret one. Now, lets get into how we can train one on our data. Recall the procedure in which a ML model is trained from Week 5:\n",
    "\n",
    "\n",
    "1. We pass all points from the dataset \\( $x$ \\) into the model.\n",
    "2. The model processes this input, performing computations based on \\( $x$ \\) and its current weights, to produce predictions, denoted as \\( $\\hat{y}$ \\).\n",
    "3. We then calculate the error (the opposite of accuracy) of all the predictions the model made.\n",
    "4. By looking at what the model got right and wrong, we can adjust the relevant parameters of the model so that the next set of predictions is more accurate.\n",
    "5. We then repeat steps 1-4 until the error stops reducing, or we reach some stopping conditions (ex. we hit a limit on the number of guesses).\n",
    "\n",
    "Decision Trees are trained in a very similar manner, starting with the root node. The algorithm can be summarized as follows:\n",
    "\n",
    "1. We pass all points from the dataset \\( $x$ \\) into the model.\n",
    "2. Using these datapoints, we create a set of candidate decision rules (`feature <= some value`) by looking at every possible combination of input features and feature values.\n",
    "3. From the candidate rules, we calculate how good each rule is at identifying the classes.\n",
    "4. We then add the rule to our tree, creating a new decision node and leaf nodes.\n",
    "5. We then repeat steps 1-4 until the error stops reducing or we reach some stopping condition.\n",
    "6. We assign each leaf node a class based on the majority class that reaches the node.\n",
    "\n",
    "\n",
    "![fkiw](dtree_flow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm, there are two main questions we need to answer:\n",
    "\n",
    "1. How do we calculate how good a rule is?\n",
    "2. How do we know when to stop?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VrTzD42JSSQ"
   },
   "source": [
    "### Impurity:\n",
    "We can measure how good a rule is by measuring the **Impurity** of the resultant groups.\n",
    "\n",
    "Impurity measures the amount of uncertainty in the data present in the data. Intuitively for a classification problem, we can understand this as \"if we were to randomly select a point, how confident are we that we know what the point will be.\"\n",
    "\n",
    "Consider the following groups:\n",
    "\n",
    "![name](groupab.png)\n",
    "\n",
    "\n",
    "If we were to randomly select a data point from Group A, we have a 2/3 chance of the point being green and a 1/3 chance of it being purple, therefore we can say we are relatively confident that a point chosen from Group A will be green. However, in Group B, there is a 50/50 chance that the point will be green or purple, therefore we are not confident that we can predict the result of randomly choosing a point. In this case, we can say that Group B is less pure or has *higher impurity*\n",
    "\n",
    "**A group has zero impurity if there is only one class in the group.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZBhwb6hjI8T"
   },
   "source": [
    "#### Splitting with Impurity\n",
    "\n",
    "A decision tree uses impurity to decide what decisions to make. At each internal node, the decision tree examines the points that have reached that node and chooses a decision rule that creates two subgroups with low amounts of impurity. Take the following dummy classification as an example:\n",
    "\n",
    "\n",
    "\n",
    "We can see that there are two classes in the dataset (orange and blue). Given these points the decision tree will generate a few candidate decision rules that split the space up. ***The tree will then choose the split that reduces impurity the most.***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClhW-wPHIDvc"
   },
   "source": [
    "---\n",
    "##### **Q2: Below we plot two different candidate splits. Which split will the decision tree choose: Candidate 1 or Candidate 2? Justify your answer**\n",
    "\n",
    "![splits](splits.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bvOTi6dJLSN"
   },
   "source": [
    "*Your Answer Here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPxhiGwN8Vz5"
   },
   "source": [
    "### When to Stop\n",
    "\n",
    "There are various conditions in which we can stop creating new splits in a decision tree, but most of the time we focus on the following four:\n",
    "\n",
    "1. **Maximum Tree Depth:** The first condition is that the node we have added has hit the maximum depth we allow.\n",
    "2. **Not Enough Samples**: The second condition is also a relatively simple condition; we cannot split up the space if there are not enough points to create two splits that have a certain number of points. For example, if we split the samples, and one split only has one sample, we may not go through with that split.\n",
    "3. **No Reduction in Impurity**: The third condition is that the resultant groups from the split are already very pure, and there is no possible reduction in impurity that will result from adding a new split.\n",
    "4. **Nowhere to go**: The tree will stop training if all samples are perfectly classified or if the samples that are not perfectly classified cannot be split from other samples (e.g. two samples with identical values, but different classes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdfQPEkhVcnt"
   },
   "source": [
    "### Fitting a Decision Tree\n",
    "Putting everything together, we fit a decision tree onto our training set in the cell below. Like `LogisticRegression`, we don't need to code the Decision Tree training procedure ourselves. Instead, we can simply use the `DecisionTreeClassifier` from `sklearn.tree`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK-GWpNpV9Dc"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an instance of the Decision Tree model.\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth = 3, # Condition 1: Stop creating new nodes when we reach a depth of 3\n",
    "    min_samples_split = 4, # Condition 2: Don't create a new node if there are less than 4 samples in a node.\n",
    "    min_impurity_decrease = 0.02, # Condition 3: Each node needs to reduce impurity by least 0.05\n",
    ")\n",
    "\n",
    "# Fit the model to the training data.\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "train_predictions = dt.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfWJ5JqXYfOC"
   },
   "source": [
    "Note, while we specify `max_depth,` `min_impurity_decrease` and `min_samples_split` here, if we do not specify a value then the tree will follow condition 4 and keep adding nodes till theres nowhere to go (condition 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaWxSJpVX677"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q3: Use the model to make predictions on the test set. What is the accuracy on the test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1Rrp24nX7Fj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "### Your Code here\n",
    "\n",
    "### Make predictions on the test set\n",
    "\n",
    "### Calculate test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qeQcvCQapsI"
   },
   "source": [
    "### Visualizing Our Tree\n",
    "\n",
    "One benefit of decision trees is that we can visualize the trained tree to gain insights into our data. Below we use the `plot_tree` function from `sklearn.tree` to visualize our model. This function takes in:\n",
    "1. The trained decision tree\n",
    "2. The names of the features (`feature_names`)\n",
    "3. The names of the classes (`class_names`)\n",
    "4. Flags to control what the visualization outputs including whether to color the boxed (`filled`) and display the impurity of the nodes (`impurity`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxq1hSIgaon6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "# increase figure size\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# run the plot_tree command\n",
    "plot_tree(dt, # pass in the trained model\n",
    "          feature_names = X.columns, # provide the feature names\n",
    "          class_names = ['tested_negative', 'tested_positive'], # provide the class names\n",
    "          filled = True, # color the nodes\n",
    "          impurity = False # do not show impurity values (makes tree easier to read)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvKDZcp0X7Mz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fudUfmAxSydP"
   },
   "source": [
    "## Overfitting and Underfitting\n",
    "In Week 5, we introduced the concept of splitting the data in order to better evaluate how the model might perform on real-world data. As a reminder, we have two sets of data:\n",
    "\n",
    "1. Training set: This is the dataset we give the model to learn from. Think of it as homework given to students. The model can practice getting these answers correctly (aka training).\n",
    "2. Test set: This is data the model has not seen. Think of it as the exam. Student's used homework questions to learn the concepts (training), now we evaluate how well they learned patterns and trends by giving them questions they have not\n",
    "\n",
    "This week, we will delve more into the reason *why* a model may underperform at a task. Specifically, a model performs poorly in situations when the model is **underfitting** and **overfitting**.\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and unseen data. For example, a decision tree can underfit when there are not enough decision nodes since it cannot make enough decisions to adequately distinguish between the classes.\n",
    "\n",
    "**Overfitting** happens when a model learns the training data too well, including its noise and small details, making it overly complex. As a result, the model performs great on the training data but poorly on unseen data because it fails to capture the broader patterns. For example, if a decision tree is too deep (too many decisions), each point could potentially end up in its own leaf, causing the model to \"memorize\" the training data instead of generalizing.\n",
    "\n",
    "### Controlling for Overfitting and Underfitting\n",
    "So how we do control for overfitting and underfitting? Simple: We configure the model's **Hyperparameters**. Hyperparameters are basically the knobs and levers of the model. They are parameters the user sets to control how the model learns. For example, in Logistic Regression, `max_iter` was a hyperparameter controlling the number of training iterations. Similarly, Decision Trees also have hyperparameters you have already seen: `max_depth`, `min_samples_split`, and `min_impurity_decrease`.\n",
    "\n",
    "* `max_depth`: The first parameter used to control overfitting is `max_depth`. Since every extra depth of the tree doubles the number of leaves, a sufficiently deep tree can easily assign each training sample to its own leaf. This over-complexity has the tendency to fit onto noise instead of actual trends.\n",
    "\n",
    "* `min_samples_split`: Another way to avoid overfitting is to specify that there must be more than one point in each leaf. This will force the tree to focus on patterns that appear multiple times, making it more robust to noise. This also helps very deep trees from assigning 1 leaf per point.\n",
    "\n",
    "* `min_impurity_decrease`: While the goal of each node is to reduce the impurity in each split, sometimes to promote generalization, we only want our decision tree to consider splits that are \"strong\" enough, i.e. reduce impurity enough. This helps in reducing overfitting to noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXhSaCg8fcFV"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q4: Train a Decision Tree with `max_depth` values between 1 and 4.  Record the train and test accuracy for each tree. At what depth do you think the tree starts to overfit? At what depths does the tree underfit? Justify your answer.**\n",
    "\n",
    "> HINT: You can use a for loop to iterate over the `max_depth` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxvXzkqSfccb"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6gA8Aoufcmk"
   },
   "source": [
    "Overfitting happens after a depth of 2 since the test accuracy starts to decrease while the train accuracy increases. The tree starts to underfit at a depth of 1 since the accuracy is low for both the train and test set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH7wrXZKdwrg"
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "While hyperparameters are important to a model's performance, we left out the answer to the key question: *How do we choose the right hyperparameters?*\n",
    "\n",
    "One way you might think to do it is to test a ton of hyperparameter configurations and choose the one that achieves the best performance on the test set. However there is a core problem here: **The test set is meant to represent unseen data.** Lets see this from the context of a ML model as a student we mentioned above: The training set samples are homework problems while the the test set is the exam. Picking hyperparameters based on the test set performance is like creating a studying strategy, taking the exam, getting your score back, changing your studying strategy, and taking the *exact same exam* over and over again. While yes, your test score will be good, but this is an **overly optimistic** score that would not reflect well on your knowledge of the subject matter. Similarly, using the test set to pick the best hyperparameters would result in an overly optimistic estimate of your model's real-world performance.\n",
    "\n",
    "So what do we do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq0Pp48C7llS"
   },
   "source": [
    "\n",
    "### K-Fold Cross-Validation\n",
    "Well, like studying for the exam, we can use *practice* exams. In ML, these practice exams are called *validation* data sets and we construct these sets by taking the training dataset, and further randomly split it up into $K$ relatively-equal sized chunks, or \"Folds\". We will then create a new version of our model, train it model using all but one of these folds, and test our model on the last fold. We will then rotate folds and repeat this procedure. At the end, we will average the performance across all the folds. This procedure is called **K-Fold Cross-Validation.** We will repeat this procedure across different configurations of hyperparameters.\n",
    "\n",
    "![gscv](grid_search_cross_validation.png)\n",
    "\n",
    "We can create these folds using the `KFold()` utility from `sklearn.model_selection.` This takes a dataset, assigns samples to different folds, and provides the indices of what samples are in each fold. Below we create 5 Folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XeVQEzLkiCV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5,\n",
    "           shuffle = True) # make sure you set shuffle to true\n",
    "fold_indices = kf.split(X_train)\n",
    "for fold_index in fold_indices:\n",
    "    # Get what samples should be in each split\n",
    "    train_fold_indices, val_fold_indices = fold_index\n",
    "\n",
    "    # select the samples\n",
    "    X_train_fold = X_train.iloc[train_fold_indices]\n",
    "    y_train_fold = y_train.iloc[train_fold_indices]\n",
    "    X_val_fold = X_train.iloc[val_fold_indices]\n",
    "    y_val_fold = y_train.iloc[val_fold_indices]\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Number of samples in training fold: {len(train_fold_indices)}\")\n",
    "    print(f\"Number of samples in validation fold: {len(val_fold_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdSLTXJPrtFM"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q5: Below we create the procedure for testing one hyperparameter configuration of a decision tree. Fill in the blank pieces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXxsVf4FrtNV"
   },
   "outputs": [],
   "source": [
    "def test_hyperparameter(X_train, y_train, max_depth, min_samples_split, min_impurity_decrease):\n",
    "  # create folds\n",
    "  kf = KFold(n_splits = 5)\n",
    "  fold_indices = kf.split(X_train)\n",
    "  accuracies = []\n",
    "  # loop through the folds\n",
    "  for fold_index in fold_indices:\n",
    "    # Get what samples should be in each split\n",
    "    train_fold_indices, val_fold_indices = fold_index\n",
    "    # select the samples\n",
    "    X_train_fold = X_train.iloc[train_fold_indices]\n",
    "    y_train_fold = y_train.iloc[train_fold_indices]\n",
    "    X_val_fold = X_train.iloc[val_fold_indices]\n",
    "    y_val_fold = y_train.iloc[val_fold_indices]\n",
    "\n",
    "\n",
    "    # initialize a new decision tree\n",
    "    dt = ... # YOUR CODE HERE\n",
    "\n",
    "    # Train the decision tree on the train fold\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "  \n",
    "    # Make predictions on the validation fold\n",
    "    val_predictions  = ... # YOUR CODE HERE\n",
    "\n",
    "    # calculate the accuracy of the val predictions\n",
    "\n",
    "    fold_accuracy = ... # YOUR CODE HERE\n",
    "\n",
    "    accuracies.append(fold_accuracy)\n",
    "  return np.mean(accuracies) # return the average accuracy across all folds\n",
    "\n",
    "\n",
    "## Run the function with a set of parameters\n",
    "cv_acc = test_hyperparameter(X_train, y_train,\n",
    "                             max_depth = 3,\n",
    "                             min_samples_split = 3,\n",
    "                             min_impurity_decrease = 0.01)\n",
    "\n",
    "print(f\"The average cross-validation accuracy is: {cv_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DvY7p3WrtkY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1q3KECf7myO"
   },
   "source": [
    "### Parameter Grid Search\n",
    "\n",
    "Now that we have the right way to evaluate a set of hyperparameters, we now need to actually search for the best parameters. We do this through something called \"Grid Search\" where we try every possible combination of parameters and see which configuration achieves the best cross-validation score. Below we test 3 different values of `max_depth`, `min_samples_split`, and `min_impurity_decrease` each for a total of $3 \\times 3 \\times 3 = 27$ combinations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM9GoA4j9tiq"
   },
   "outputs": [],
   "source": [
    "for max_depth in [1,2,3]:\n",
    "  for min_samples_split in [2,3,4]:\n",
    "    for impurity in [0.00, 0.01, 0.02]:\n",
    "      cv_acc = test_hyperparameter(X_train, y_train,\n",
    "                                   max_depth = max_depth,\n",
    "                                   min_samples_split = min_samples_split,\n",
    "                                   min_impurity_decrease = impurity)\n",
    "      print(f\"max_depth: {max_depth}, min_samples_split: {min_samples_split}, min_impurity_decrease: {impurity}, cv_acc: {cv_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8IT8fCg-G1M"
   },
   "source": [
    "After finding the best set of parameters, we then train the model on *all* the training data and do our final evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb-1C-8L-Rnk"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q6: What set of parameters had the best CV accuracy? Train a model using these parameters using the entire training set. Report the training and testing accuracy.**\n",
    "\n",
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5j9XCBp8-G6e"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWhbwYtC-HAK"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saRl57mhd0WF"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In conclusion, decision trees are powerful and intuitive tools in machine learning that offer both flexibility and interpretability. By iteratively splitting data based on feature values, decision trees can model complex relationships and make predictions for both classification and regression tasks. Additionally, the ability of a tree to be visualized and easily interpreted makes them a powerful analytic tool in your toolbox.\n",
    "\n",
    "However, with this flexibility comes the risk of overfitting, where the model becomes too tailored to the training data and loses its ability to generalize to new data. To prevent this, various parameters can be tested using K-Fold Cross Validation to find the best configuration. With this knowledge, you can confidently apply ML to a wide range of problems, making them a valuable tool in your machine learning toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_q_LHuz_YKF"
   },
   "source": [
    "## Graded Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhFQ3T1c_Ys0"
   },
   "source": [
    "---\n",
    "\n",
    "##### **GQ1: Read in the heart failures dataset (`hf_data_tut.csv`) (1pt). Split the predictor variables (features) from the labels (response variable) (1pt), and create a training and testing set (1pt). **\n",
    "\n",
    "> HINT: Look at your work from Week 3 and 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlD5XdvE_a9J"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNT5bjiB_bA0"
   },
   "source": [
    "---\n",
    "##### **GQ2: Train a decision tree of Depth 2 and visualize the tree (1pt). What feature is used in the root node (1pt)? How many leaves does the tree have (1pt)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LGtV68b_gMO"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwZIK6ZuA6QG"
   },
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv9CbQn7Ayjd"
   },
   "source": [
    "---\n",
    "##### **GQ3: Print the testing and training accuracy of the model above (1pt). Do you think the model is under or overfitting (or neither) and why (2pt)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR0eUDh9A40M"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVm2Csr_A44z"
   },
   "source": [
    "Train Acc is significantly higher than test accuracy, therefore the model is probably overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs0rKJsC_gP6"
   },
   "source": [
    "---\n",
    "##### **GQ4: Search for the best configuration of Hyperparameters for the Decision Tree on this dataset. What is the best CV Accuracy (3pt)? For the best configuration, what is the testing and training accuracy and how does this compare with the logistic regression model you trained? (2pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6umb92HA87G"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
