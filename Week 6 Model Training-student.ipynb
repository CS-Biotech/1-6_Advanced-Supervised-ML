{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nULkunmntyNF"
   },
   "source": [
    "# Week 6: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Module\n",
    "\n",
    "This week, having laid the groundwork with an understanding of what logistic regression is and its role in predicting categorical outcomes, we are now ready to focus exclusively on how to train a logistic regression model effectively.\n",
    "\n",
    "Training is a crucial phase where the model is exposed to data, allowing it to learn the relationship between input features and the target variable. This stage is critical for applications in biology, where logistic regression can be instrumental in tasks like predicting the presence of a genetic trait, determining the likelihood of a medical condition, or classifying species in ecological studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "Throughout this module, we will concentrate on the following aspects of the model training process:\n",
    "\n",
    "1. Loss Function\n",
    "2. Gradient Descent\n",
    "3. Validation Set\n",
    "4. Hyperparameter Tuning\n",
    "\n",
    "By focusing on these key areas, you will gain a deeper understanding of the logistic regression training process and how to apply it to biological data using Python. Let's embark on this detailed exploration of model training within the fascinating intersection of machine learning and biology. Below is the code from last week, preprocessing and reaching the training portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZnsa9s0tyNH",
    "outputId": "9c402289-c698-48d3-f905-79aead562226"
   },
   "outputs": [],
   "source": [
    "# Data science\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', index_col=0)\n",
    "\n",
    "print(f\"Shape of original dataset: {df.shape}\")\n",
    "\n",
    "# Data cleaning\n",
    "# Encode target feature to binary class and split target/predictor vars\n",
    "y = df[\"diagnosis\"].map({\"B\" : 0, \"M\" : 1})\n",
    "x = df.drop(\"diagnosis\", axis = 1)\n",
    "\n",
    "# Drop all \"worst\" columns\n",
    "cols = ['radius_worst',\n",
    "        'texture_worst',\n",
    "        'perimeter_worst',\n",
    "        'area_worst',\n",
    "        'smoothness_worst',\n",
    "        'compactness_worst',\n",
    "        'concavity_worst',\n",
    "        'concave points_worst',\n",
    "        'symmetry_worst',\n",
    "        'fractal_dimension_worst']\n",
    "x = x.drop(cols, axis=1)\n",
    "\n",
    "# Drop perimeter and area (keep radius)\n",
    "cols = ['perimeter_mean',\n",
    "        'perimeter_se',\n",
    "        'area_mean',\n",
    "        'area_se']\n",
    "x = x.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJO8e0TQtyNI"
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go into how we will train our model, let's specify what we are training our model for.\n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm in machine learning, primarily used to minimize a function, which in this context is the <span style=\"background-color: #AFEEEE\">**loss function**</span>. The loss function, often synonymous with the objective or cost function, is a crucial component in this process. They are mathematical formulations used to quantify the difference between the predicted output of a model and the actual data. Or in other words, it evaluates the performance of the model for each input, quantifying how 'off' or 'wrong' the model's predictions are compared to the actual values. The goal of gradient descent is to minimize this function, thereby improving the model's accuracy. It may help to imagine the loss function as a hill, where you want to slowly traverse it based on its slope to find the lowest valley.\n",
    "\n",
    "<img src=\"loss_func.jpeg\" alt=\"Gradient descent steps towards loss function minimum\" style=\"width: 600px;\" class=\"center\"/>\n",
    "\n",
    "<a href=\"https://blog.gopenai.com/understanding-of-gradient-descent-intuition-and-implementation-b1f98b3645ea\">Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKmN7ZcgtyNI"
   },
   "source": [
    "### Understanding Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of you might already have a solid foundation in statistics and are familiar with generalized mixed models, including logistic regression, which extends from linear regression. However, the approach we're going to explore here differs slightly. Don't worry, those who have not taken these courses can also grasp the basic underlying steps that we will be highlighting here. When you learned logistic regression in statistics, the focus was primarily on computing the closed-form solutions directly. These solutions provide exact answers derived from the mathematical equations of the model. However, it's important to recognize that computing closed-form solutions isn't always feasible or desirable, especially in certain contexts.\n",
    "\n",
    "For instance, when dealing with large datasets, finding these closed-form solutions can be computationally expensive and time-consuming. Moreover, in the realm of many machine learning models, closed-form solutions may not even exist due to the complexity and high dimensionality of the data. This is where alternative methods like <span style=\"background-color: #AFEEEE\">**gradient descent**</span> come into play. The use of gradient descent makes the training process adaptable and scalable for high-dimensional data, which is often the case in biological datasets that can include thousands of features from genetic markers to pixel data from medical images.\n",
    "\n",
    "The process of gradient descent involves iteratively taking steps towards the minimum of the loss function. In the end, we have a model with its weight values set based on context from our dataset. However, to get there, each step through the loss function in this process has two essential aspects: its direction and its size.\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">-  **Direction:**</span> The direction of each step is determined by the gradient of the function at the current point. The gradient is a vector that points in the direction of the steepest ascent â€“ the direction in which the function increases most rapidly. However, since the goal is to minimize the function, gradient descent takes steps in the opposite direction of the gradient. By moving against the gradient, the algorithm descends towards the function's minimum.\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">-  **Size:**</span> The size of each step is proportional to the gradient. If the steps are too large, the algorithm might overshoot the minimum; if they are too small, the descent could be very slow or get stuck in a local minimum. The speed at which the model \"learns\" needs to be carefully chosen to ensure efficient and effective convergence to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqV2opRdtyNJ"
   },
   "source": [
    "### Working with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively utilize gradient descent, a significant part of our task involves determining the optimal frequency and magnitude of the steps we take. This brings us to the concept of <span style=\"background-color: #AFEEEE\">**hyperparameters**</span>. Unlike model weights/parameters, which are learned from the data, hyperparameters are settings that we must define before training the model. All machine learning models have hyperparameters, with two common ones being the <span style=\"background-color: #AFEEEE\">**learning rate**</span> and the <span style=\"background-color: #AFEEEE\">**number of epochs**</span>.\n",
    "\n",
    "In essence, gradient descent works by iteratively moving in the direction opposite to the gradient, with the magnitude of each step influenced by the learning rate. This iterative process continues until the algorithm <span style=\"background-color: #AFEEEE\">**converges**</span>, which occurs when additional steps fail to significantly reduce the value of the loss function, suggesting that the minimum (or a local minimum) has been reached. Determining the step size and the total number of steps involves setting specific hyperparameters.\n",
    "\n",
    "- <span style=\"background-color: #AFEEEE\">**Learning Rate (Alpha)**</span>: The learning rate, often denoted as alpha, controls the size of each step. A lower learning rate results in smaller steps, and a higher rate leads to larger steps. The choice of learning rate is crucial as it affects the performance of the model, influencing how quickly or slowly it converges to the minimum.\n",
    "\n",
    "- <span style=\"background-color: #AFEEEE\">**Epochs**</span>: An epoch, essentially an iteration, represents one complete pass through the entire dataset. The number of epochs determines how many times the model will update its weights/parameters using the gradient from the loss function. Setting this number too low might prevent the model from reaching the minimum, while a very high number could lead to unnecessarily long training times. It's important to choose a value that allows the model to reach the minimum efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nngDGF9EtyNJ"
   },
   "source": [
    "In practical terms, when working with a tool like `SGDClassifier()`, you'll encounter object parameters such as `alpha` and `max_iter`. These represent the hyperparameters we can control: \n",
    "- `alpha` specifies the learning rate. An example of this implementation was seen in week 5 with our analysis of the breast cancer dataset.\n",
    "- `max_iter` sets the number of epochs, dictating how many times the model will iterate over the dataset. \n",
    "\n",
    "Here, explicitly, we are using an alpha/learning rate of 0.00001 and 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gyn_E5FDtyNJ"
   },
   "source": [
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| .fit() | Numpy array(s) | returns a History object containing training and validation metrics | model.fit(x_train, y_train) |\n",
    "| .score() | Numpy array(s) | returns a performance metric that is specific to the type of model | model.score(x_test, y_test) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECZDPdR2tyNJ"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into a training and a validation dataset\n",
    "test_ratio = 0.15\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMmbzy2dtyNJ",
    "outputId": "fb990da9-204f-4dbf-c535-dfbec1dc216f"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SGDClassifier(loss='log_loss', alpha=1e-5, max_iter=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_predictions = model.predict(x_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxYbMflhtyNK"
   },
   "source": [
    "It seems our current model's performance is not as high as we hoped, returning a poor accuracy. This could be a sign that our model's hyperparameters need adjusting. For our case, it seems like the model may benefit from additional training time or a different learning rate. You might have also received a warning from sklearn specifically stating so.\n",
    "\n",
    "Before we move on, run the previous cell a few more times. You may see that the accuracy produced seems random. This is because we may start at various random locations on this gradient; the weights that are initialized are random.\n",
    "\n",
    "**Q1. In the code snippet below, experiment with different values for `alpha` and `max_iter` until you get an accuracy above 80% on the test set.**\n",
    "\n",
    "> HINT: Copy the code from the code cell above.\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emptmvOTtyNK"
   },
   "source": [
    "You might feel a sense of accomplishment after achieving good accuracy on the test set, but this could be a case of <span style=\"background-color: #AFEEEE\">**overfitting**</span> (we'll learn more about overfitting and underfitting in Week 7). Hence, the question arises: how should we test the effectiveness of our hyperparameter tuning without falling into this trap? It's crucial to understand that using the test set for evaluating hyperparameter choices can inadvertently lead to overfitting on the test dataset itself. This happens because, even though we're not explicitly including the test set in the training process, we're still allowing the test data to influence our model's training indirectly through our decisions.\n",
    "\n",
    "To avoid this pitfall, we need a different strategy. Instead of relying on the test set for tuning hyperparameters, we should partition our data differently. We'll set aside a portion of our training set to create what's known as a <span style=\"background-color: #AFEEEE\">**validation set**</span>. This validation set acts as a pseudo-test set, allowing us to evaluate our model and its hyperparameters without compromising the integrity of the actual test set. By using this approach, we ensure that our model is tuned effectively without being biased by the test data, leading to a more robust and generalizable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP7P_WLUtyNK"
   },
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validating our trained model, we require some data points from the training set. We can't split with data from the test set because the test set is meant to be a completely independent subset of data used to evaluate the final performance of the model (it also only contains a low number of samples from the original dataset). The test set should mimic new, unseen data that the model may encounter in a real-world scenario. Hence, it should not influence the model training or tuning process in any way.\n",
    "\n",
    "The main function of a validation set is to offer a fair assessment of a model that's been trained on a dataset, especially during the adjustment of its hyperparameters. Thus, we further remove another subset of our training set to create this new validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCsvs2Q_tyNK"
   },
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "# train (70%), val (15%), test (15%)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "\n",
    "# Splitting the training from the validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_ratio/(train_ratio + val_ratio), random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXq7X4JhtyNK"
   },
   "source": [
    "It is important that the validation set is kept separate from the training set and is not used in the training process (you won't get a good grasp of your knowledge if you view the mock exam beforehand). This separation ensures that the validation performance is a good indicator of how well the model generalizes to new, unseen data. The test set is then used as the final performance metric, as it remains untouched and completely independent throughout the model development process.\n",
    "\n",
    "<img src=\"data_split.png\" alt=\"Orginial data vs data when it is split into train, validation, and test sets\" style=\"width: 600px;\" class=\"center\"/>\n",
    "\n",
    "\n",
    "Once these three sets are made, we can now train our model over a series of iterations until we are confident it may perform well on our test set (i.e., new unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEmlUmNvtyNK"
   },
   "source": [
    "**Q2. Let's experiment with different numbers of epochs/iterations. Specifically, we will test what happens if we were to increase the number from 5 epochs to 10, 50, and 100. Below, there are three code cells for you to fill out. Copy the code above and change the `max_iter` parameter. Remember to set `random_state` to 1**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIJoehPMtyNL"
   },
   "outputs": [],
   "source": [
    "# Max iterations of 10\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDQvQMPItyNL"
   },
   "outputs": [],
   "source": [
    "# Max iterations of 50\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhLIvFKttyNL"
   },
   "outputs": [],
   "source": [
    "#Max iterations of 100\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvUddNK-tyNL"
   },
   "source": [
    "**Q3. After making these changes, observe how the test accuracy has shifted. Did it improve? If so, explain why conceptually you think that increasing the number of epochs helped in this case. What oddity do you observe when you compare 50 to 100 epochs? Why may that be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLua5i9EtyNL"
   },
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n",
    ".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now shift our focus to examining the impact of varying the learning rate. By experimenting with significantly high and low values of the learning rate, we can observe how taking steps that are either too large or too small affects our training process. This exploration will help us understand the consequences of extreme adjustments in the learning rate.\n",
    "\n",
    "Although the point of machine learning is to have a model \"learn\" the patterns in our dataset for us, there are still many parts of the process under our control; hence, there is always room for human error and for us to improve. An issue that frustrates most beginners is hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB2udVamtyNL"
   },
   "source": [
    "### Misunderstanding the Learning Rate\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">- **Scale and Impact**</span>: A common misconception is underestimating the impact of its scale. For instance, a learning rate of 0.1 is not just slightly larger than 0.01; it's actually ten times larger. This significant increase can lead to very different training behaviors. A too high learning rate might cause the model to overshoot the minimum, while a too low learning rate might result in slow convergence or getting stuck in local minima.\n",
    "<span style=\"background-color: #AFEEEE\">- **Finding the Right Balance**</span>: Beginners should start with a moderate learning rate and adjust based on the performance. Visualization of the loss function over training iterations can be helpful in tuning this parameter. Below is a graphical example of gradient descent, and what the process would look like if you increased the learning rate for a set number of epochs to too high.\n",
    "\n",
    "<img src=\"lr_effect.png\" alt=\"Choice of learning rate and impact on gradient descent\" style=\"width: 600px;\" class=\"center\"/>\n",
    "\n",
    "Just because you have identified that your model is underfitting due to a low learning rate, be careful not to set the learning rate too high. To find the godilocks value, be patient and make sensible small adjustments.\n",
    "\n",
    "**Q4. Below in the two cells, examine what happens when you set the `alpha` value to 1 and 1e-1, respectively. You may keep the `max_iterations` as 100.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwgAvLbctyNL"
   },
   "outputs": [],
   "source": [
    "# Alpha value of 1\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVYN7FxStyNL"
   },
   "outputs": [],
   "source": [
    "# Alpha value of 0.1\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXW4XoS6tyNM"
   },
   "source": [
    "**Q5. What are the results that you see when changing the alpha value? If the learning rate is too small, what can you do to make sure that the model converges (besides changing the alpha value)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0cUaol7tyNM"
   },
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KakTuo5tyNM"
   },
   "source": [
    "### Final Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have reviewed the complete process of training our model and tuning hyperparameters, it's finally time for us to evaluate our models on the test dataset. Below, predict on the test set (not the validation set) and see how it performs.\n",
    "\n",
    "**Q6. As we have altered and explored different hyperparameter values for our model, retrain the model below with what you believe are the best hyperparameter values and evaluate your model on the test dataset.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8blr1HztyNM"
   },
   "outputs": [],
   "source": [
    "# Load the model with the best hyperparameters from our experiments\n",
    "\n",
    "# Fit the model on the training data\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_predictions = ...\n",
    "val_accuracy = ...\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_predictions = ...\n",
    "test_accuracy = ...\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3x_0ZGityNM"
   },
   "source": [
    "## **Graded Exercises: (7 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ1: Read in the heart failures dataset (`hf_data_tut.csv`) (1 mark) and split the predictor variables (features) from the labels (response variable) (1 mark).**\n",
    "\n",
    "> HINT: Look at your work from Week 3.\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18aCqYmRtyNM"
   },
   "source": [
    "**GQ2: Train an `SGDClassifier()` model on *all* the data (1 mark). What is the accuracy (1 mark)?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXdx7fw7tyNM"
   },
   "outputs": [],
   "source": [
    "...\n",
    "accuracy = ...\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ3: Split the dataset into a train and test set and retrain your model (1 mark). What is the train and test accuracy (2 marks)?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "# Retrain model\n",
    "\n",
    "# Training accuracy\n",
    "train_accuracy = ...\n",
    "print(f\"Train accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "# Testing accuracy\n",
    "test_accuracy = ...\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwvXcu3ktyNN"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this module, we explored the concept of \"training\" as it applies to logistic regression models. We delved into the gradient descent method, which is a common technique used by various machine learning models, including logistic regression, to minimize their loss function. This mathematical representation quantifies how \"correct\" our model's predictions are. Additionally, we discussed hyperparameters, which are variables set by us, the users, that influence the training process. This contrasts with the weights determined by the model during training. We also examined the role of a validation set. This set comprises data separated from the training dataset, used to refine hyperparameters effectively without relying on the test set, thereby avoiding the risk of overfitting. Lastly, we addressed the challenges associated with tuning hyperparameters, particularly the impact of choosing suboptimal learning rates and epoch numbers on the model's performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
